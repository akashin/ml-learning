{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "cell_size = 10\n",
    "\n",
    "with tf.device(\"gpu:0\"):\n",
    "    net = LSTM()\n",
    "    output_net = tf.keras.layers.Dense(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.constant(np.zeros((batch_size, input_size)), dtype=tf.float32)\n",
    "\n",
    "# nh, nc = net([x, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xor_batch(batch_size=32):\n",
    "    x_batch = np.random.randint(2, size=(batch_size, 50, 1))\n",
    "    \n",
    "    def xor(sequence):\n",
    "        result = []\n",
    "        accum = 0\n",
    "        for v in sequence:\n",
    "            accum = accum ^ v\n",
    "            result.append(accum)\n",
    "        return result\n",
    "        \n",
    "    y_batch = np.array(list(xor(x) for x in x_batch))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def generate_xor_last_batch(batch_size=32):\n",
    "    x_batch = np.random.randint(2, size=(batch_size, 50, 1))\n",
    "    \n",
    "    def xor(sequence):\n",
    "        result = []\n",
    "        accum = 0\n",
    "        for v in sequence:\n",
    "            accum = accum ^ v\n",
    "            result.append(accum)\n",
    "        return result\n",
    "        \n",
    "    y_batch = np.array(list(xor(x) for x in x_batch))\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(net, xs): \n",
    "    h = tf.constant(np.zeros((batch_size, hidden_size)), dtype=tf.float32)\n",
    "    c = tf.constant(np.zeros((batch_size, cell_size)), dtype=tf.float32)\n",
    "    \n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        h, c = net([x, h, c])\n",
    "        ys.append(output_net(h))\n",
    "        \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 0.6931909322738647\n",
      "It: 100, Loss: 0.6930636763572693\n",
      "It: 200, Loss: 0.6928489804267883\n",
      "It: 300, Loss: 0.6922973394393921\n",
      "It: 400, Loss: 0.6933772563934326\n",
      "It: 500, Loss: 0.6928486824035645\n",
      "It: 600, Loss: 0.691215991973877\n",
      "It: 700, Loss: 0.690264105796814\n",
      "It: 800, Loss: 0.6842941045761108\n",
      "It: 900, Loss: 0.6811379790306091\n",
      "It: 1000, Loss: 0.6769529581069946\n",
      "It: 1100, Loss: 0.6707047820091248\n",
      "It: 1200, Loss: 0.668885350227356\n",
      "It: 1300, Loss: 0.6497145295143127\n",
      "It: 1400, Loss: 0.20364966988563538\n",
      "It: 1500, Loss: 0.11225822567939758\n",
      "It: 1600, Loss: 0.06564738601446152\n",
      "It: 1700, Loss: 0.04092329367995262\n",
      "It: 1800, Loss: 0.02956956811249256\n",
      "It: 1900, Loss: 0.022227099165320396\n",
      "It: 2000, Loss: 0.01788059063255787\n",
      "It: 2100, Loss: 0.01480279304087162\n",
      "It: 2200, Loss: 0.012501670978963375\n",
      "It: 2300, Loss: 0.00997020024806261\n",
      "It: 2400, Loss: 0.008596899919211864\n",
      "It: 2500, Loss: 0.00825677439570427\n",
      "It: 2600, Loss: 0.007583946455270052\n",
      "It: 2700, Loss: 0.006588848773390055\n",
      "It: 2800, Loss: 0.005852062720805407\n",
      "It: 2900, Loss: 0.00519485492259264\n",
      "It: 3000, Loss: 0.005325844511389732\n",
      "It: 3100, Loss: 0.004015656188130379\n",
      "It: 3200, Loss: 0.004060933832079172\n",
      "It: 3300, Loss: 0.004554461687803268\n",
      "It: 3400, Loss: 0.003873203182592988\n",
      "It: 3500, Loss: 0.0035445743706077337\n",
      "It: 3600, Loss: 0.00295065063983202\n",
      "It: 3700, Loss: 0.0029930437449365854\n",
      "It: 3800, Loss: 0.0032107883598655462\n",
      "It: 3900, Loss: 0.002516048029065132\n",
      "It: 4000, Loss: 0.002568075666204095\n",
      "It: 4100, Loss: 0.002120241755619645\n",
      "It: 4200, Loss: 0.0020853725727647543\n",
      "It: 4300, Loss: 0.0018536814022809267\n",
      "It: 4400, Loss: 0.00194099813234061\n",
      "It: 4500, Loss: 0.0017199547728523612\n",
      "It: 4600, Loss: 0.0018891033250838518\n",
      "It: 4700, Loss: 0.0014034936903044581\n",
      "It: 4800, Loss: 0.001665694173425436\n",
      "It: 4900, Loss: 0.0013282124418765306\n",
      "It: 5000, Loss: 0.0013405292993411422\n",
      "It: 5100, Loss: 0.0014237011782824993\n",
      "It: 5200, Loss: 0.0013089522253721952\n",
      "It: 5300, Loss: 0.0010215840302407742\n",
      "It: 5400, Loss: 0.0009838762925937772\n",
      "It: 5500, Loss: 0.0010499876225367188\n",
      "It: 5600, Loss: 0.0008273470448330045\n",
      "It: 5700, Loss: 0.0006883479654788971\n",
      "It: 5800, Loss: 0.0008400321239605546\n",
      "It: 5900, Loss: 0.0008217324502766132\n",
      "It: 6000, Loss: 0.0007122044917196035\n",
      "It: 6100, Loss: 0.0006875902181491256\n",
      "It: 6200, Loss: 0.000583660090342164\n",
      "It: 6300, Loss: 0.0007473738514818251\n",
      "It: 6400, Loss: 0.0005149205680936575\n",
      "It: 6500, Loss: 0.0005258414312265813\n",
      "It: 6600, Loss: 0.0005831059534102678\n",
      "It: 6700, Loss: 0.0003740639367606491\n",
      "It: 6800, Loss: 0.00047778227599337697\n",
      "It: 6900, Loss: 0.0004444457881618291\n",
      "It: 7000, Loss: 0.0003584590449463576\n",
      "It: 7100, Loss: 0.00030378976953215897\n",
      "It: 7200, Loss: 0.00029561560950241983\n",
      "It: 7300, Loss: 0.0002963391598314047\n",
      "It: 7400, Loss: 0.0002779208589345217\n",
      "It: 7500, Loss: 0.00026261943276040256\n",
      "It: 7600, Loss: 0.00032649963395670056\n",
      "It: 7700, Loss: 0.00026033155154436827\n",
      "It: 7800, Loss: 0.00024310560547746718\n",
      "It: 7900, Loss: 0.0002815182087942958\n",
      "It: 8000, Loss: 0.00025713484501466155\n",
      "It: 8100, Loss: 0.00019969862478319556\n",
      "It: 8200, Loss: 0.00019852504192385823\n",
      "It: 8300, Loss: 0.000182127405423671\n",
      "It: 8400, Loss: 0.00018286664271727204\n",
      "It: 8500, Loss: 0.00021438639669213444\n",
      "It: 8600, Loss: 0.00015648762928321958\n",
      "It: 8700, Loss: 0.0001506097469246015\n",
      "It: 8800, Loss: 0.00013076854520477355\n",
      "It: 8900, Loss: 0.0001585196005180478\n",
      "It: 9000, Loss: 0.00014648132491856813\n",
      "It: 9100, Loss: 0.00013365934137254953\n",
      "It: 9200, Loss: 0.0001289227366214618\n",
      "It: 9300, Loss: 0.0001313721586484462\n",
      "It: 9400, Loss: 9.685693657957017e-05\n",
      "It: 9500, Loss: 9.068592771654949e-05\n",
      "It: 9600, Loss: 8.94204931682907e-05\n",
      "It: 9700, Loss: 0.00011875093332491815\n",
      "It: 9800, Loss: 8.325801172759384e-05\n",
      "It: 9900, Loss: 8.496362715959549e-05\n",
      "It: 10000, Loss: 7.993800682015717e-05\n",
      "It: 10100, Loss: 7.417636516038328e-05\n",
      "It: 10200, Loss: 7.382508192677051e-05\n",
      "It: 10300, Loss: 7.499373896280304e-05\n",
      "It: 10400, Loss: 6.286764983087778e-05\n",
      "It: 10500, Loss: 7.29053863324225e-05\n",
      "It: 10600, Loss: 7.150489545892924e-05\n",
      "It: 10700, Loss: 5.929526378167793e-05\n",
      "It: 10800, Loss: 3.513253614073619e-05\n",
      "It: 10900, Loss: 5.707434320356697e-05\n",
      "It: 11000, Loss: 5.3516774642048404e-05\n",
      "It: 11100, Loss: 4.56734087492805e-05\n",
      "It: 11200, Loss: 4.27253071393352e-05\n",
      "It: 11300, Loss: 3.7148136470932513e-05\n",
      "It: 11400, Loss: 3.869834108627401e-05\n",
      "It: 11500, Loss: 3.457824277575128e-05\n",
      "It: 11600, Loss: 3.958709567086771e-05\n",
      "It: 11700, Loss: 3.3771313610486686e-05\n",
      "It: 11800, Loss: 3.910433588316664e-05\n",
      "It: 11900, Loss: 2.718125506362412e-05\n",
      "It: 12000, Loss: 2.8243206543265842e-05\n",
      "It: 12100, Loss: 3.294510315754451e-05\n",
      "It: 12200, Loss: 2.3169744963524863e-05\n",
      "It: 12300, Loss: 1.844355938374065e-05\n",
      "It: 12400, Loss: 2.3592618163092993e-05\n",
      "It: 12500, Loss: 1.9804065232165158e-05\n",
      "It: 12600, Loss: 2.153710011043586e-05\n",
      "It: 12700, Loss: 2.3374033844447695e-05\n",
      "It: 12800, Loss: 2.0057048459420912e-05\n",
      "It: 12900, Loss: 1.9401273675612174e-05\n",
      "It: 13000, Loss: 1.761926614562981e-05\n",
      "It: 13100, Loss: 1.393310412822757e-05\n",
      "It: 13200, Loss: 1.7115629816544242e-05\n",
      "It: 13300, Loss: 1.528639768366702e-05\n",
      "It: 13400, Loss: 1.3209606549935415e-05\n",
      "It: 13500, Loss: 1.3214339560363442e-05\n",
      "It: 13600, Loss: 1.46996808325639e-05\n",
      "It: 13700, Loss: 1.2033933671773411e-05\n",
      "It: 13800, Loss: 1.1127983270853292e-05\n",
      "It: 13900, Loss: 9.22097751754336e-06\n",
      "It: 14000, Loss: 1.0799862138810568e-05\n",
      "It: 14100, Loss: 1.0040467714134138e-05\n",
      "It: 14200, Loss: 1.051580511557404e-05\n",
      "It: 14300, Loss: 7.087407993822126e-06\n",
      "It: 14400, Loss: 1.1715404980350286e-05\n",
      "It: 14500, Loss: 8.15912881080294e-06\n",
      "It: 14600, Loss: 8.082311978796497e-06\n",
      "It: 14700, Loss: 7.312535217351979e-06\n",
      "It: 14800, Loss: 7.413655566779198e-06\n",
      "It: 14900, Loss: 6.399394351319643e-06\n",
      "It: 15000, Loss: 7.137414741009707e-06\n",
      "It: 15100, Loss: 8.306851668749005e-06\n",
      "It: 15200, Loss: 4.623558652383508e-06\n",
      "It: 15300, Loss: 5.876648174307775e-06\n",
      "It: 15400, Loss: 4.468373390409397e-06\n",
      "It: 15500, Loss: 4.389568857732229e-06\n",
      "It: 15600, Loss: 4.389994501252659e-06\n",
      "It: 15700, Loss: 3.871572971547721e-06\n",
      "It: 15800, Loss: 3.6782969345949823e-06\n",
      "It: 15900, Loss: 3.6514477415039437e-06\n",
      "It: 16000, Loss: 2.2808544599683955e-06\n",
      "It: 16100, Loss: 3.4578790746309096e-06\n",
      "It: 16200, Loss: 4.914067631034413e-06\n",
      "It: 16300, Loss: 3.8595667319896165e-06\n",
      "It: 16400, Loss: 2.6639534098649165e-06\n",
      "It: 16500, Loss: 3.668564886538661e-06\n",
      "It: 16600, Loss: 3.973804268753156e-06\n",
      "It: 16700, Loss: 2.500848495401442e-06\n",
      "It: 16800, Loss: 2.5950598683266435e-06\n",
      "It: 16900, Loss: 2.5306974293926032e-06\n",
      "It: 17000, Loss: 2.262567022626172e-06\n",
      "It: 17100, Loss: 2.4049068088061176e-06\n",
      "It: 17200, Loss: 1.994440935959574e-06\n",
      "It: 17300, Loss: 2.108097532982356e-06\n",
      "It: 17400, Loss: 1.8908879155787872e-06\n",
      "It: 17500, Loss: 1.7850968561106129e-06\n",
      "It: 17600, Loss: 2.327480387975811e-06\n",
      "It: 17700, Loss: 1.3650911796503351e-06\n",
      "It: 17800, Loss: 2.140148581020185e-06\n",
      "It: 17900, Loss: 1.7797417513065739e-06\n",
      "It: 18000, Loss: 1.4811722621743684e-06\n",
      "It: 18100, Loss: 1.3127585134498077e-06\n",
      "It: 18200, Loss: 1.290371301365667e-06\n",
      "It: 18300, Loss: 1.4455637256105547e-06\n",
      "It: 18400, Loss: 1.178990601147234e-06\n",
      "It: 18500, Loss: 1.1285899290669477e-06\n",
      "It: 18600, Loss: 1.1357803941791644e-06\n",
      "It: 18700, Loss: 1.0531562111282256e-06\n",
      "It: 18800, Loss: 1.3200682360547944e-06\n",
      "It: 18900, Loss: 7.977572522577248e-07\n",
      "It: 19000, Loss: 9.017284696710703e-07\n",
      "It: 19100, Loss: 6.300862196439994e-07\n",
      "It: 19200, Loss: 9.232612683263142e-07\n",
      "It: 19300, Loss: 9.064237929123919e-07\n",
      "It: 19400, Loss: 9.501956128588063e-07\n",
      "It: 19500, Loss: 8.38886478504719e-07\n",
      "It: 19600, Loss: 7.139052513593924e-07\n",
      "It: 19700, Loss: 8.02305294200778e-07\n",
      "It: 19800, Loss: 6.780691705898789e-07\n",
      "It: 19900, Loss: 5.790527666249545e-07\n",
      "It: 20000, Loss: 6.344471898955817e-07\n",
      "It: 20100, Loss: 8.400065212299523e-07\n",
      "It: 20200, Loss: 5.285016868583625e-07\n",
      "It: 20300, Loss: 5.640407039209094e-07\n",
      "It: 20400, Loss: 4.0445121385346283e-07\n",
      "It: 20500, Loss: 5.730190650865552e-07\n",
      "It: 20600, Loss: 5.171031034478801e-07\n",
      "It: 20700, Loss: 3.535273265242722e-07\n",
      "It: 20800, Loss: 3.5099424167128745e-07\n",
      "It: 20900, Loss: 4.405495701576001e-07\n",
      "It: 21000, Loss: 5.23772030192049e-07\n",
      "It: 21100, Loss: 3.404147719265893e-07\n",
      "It: 21200, Loss: 3.4052661135319795e-07\n",
      "It: 21300, Loss: 2.693741407711059e-07\n",
      "It: 21400, Loss: 3.1214025852932537e-07\n",
      "It: 21500, Loss: 3.197398541487928e-07\n",
      "It: 21600, Loss: 3.183988894761569e-07\n",
      "It: 21700, Loss: 3.173558411617705e-07\n",
      "It: 21800, Loss: 2.2556523049388488e-07\n",
      "It: 21900, Loss: 2.331275936739985e-07\n",
      "It: 22000, Loss: 2.879636156194465e-07\n",
      "It: 22100, Loss: 2.4557004962844076e-07\n",
      "It: 22200, Loss: 2.3014749217509234e-07\n",
      "It: 22300, Loss: 2.3607071852893569e-07\n",
      "It: 22400, Loss: 1.8197977169620572e-07\n",
      "It: 22500, Loss: 1.7542332386710768e-07\n",
      "It: 22600, Loss: 2.304456643287267e-07\n",
      "It: 22700, Loss: 1.78999599143026e-07\n",
      "It: 22800, Loss: 1.8633843978932418e-07\n",
      "It: 22900, Loss: 1.6357699905711343e-07\n",
      "It: 23000, Loss: 1.1749533967986281e-07\n",
      "It: 23100, Loss: 1.4539767789756297e-07\n",
      "It: 23200, Loss: 1.5258748931046284e-07\n",
      "It: 23300, Loss: 1.3485518479683378e-07\n",
      "It: 23400, Loss: 1.5791466978498647e-07\n",
      "It: 23500, Loss: 1.0855472254434062e-07\n",
      "It: 23600, Loss: 1.0929979765705866e-07\n",
      "It: 23700, Loss: 1.1935804877793998e-07\n",
      "It: 23800, Loss: 9.246153354069975e-08\n",
      "It: 23900, Loss: 7.145094826910281e-08\n",
      "It: 24000, Loss: 6.426115106705765e-08\n",
      "It: 24100, Loss: 9.663386890679249e-08\n",
      "It: 24200, Loss: 9.246154775155446e-08\n",
      "It: 24300, Loss: 7.700162285573242e-08\n",
      "It: 24400, Loss: 7.025887782674545e-08\n",
      "It: 24500, Loss: 7.52135136394827e-08\n",
      "It: 24600, Loss: 7.882703556560955e-08\n",
      "It: 24700, Loss: 8.366991721686645e-08\n",
      "It: 24800, Loss: 6.262204976792418e-08\n",
      "It: 24900, Loss: 5.111091994081107e-08\n",
      "It: 25000, Loss: 6.556502540888687e-08\n",
      "It: 25100, Loss: 6.292007270758404e-08\n",
      "It: 25200, Loss: 6.090843385209155e-08\n",
      "It: 25300, Loss: 3.5241214391135145e-08\n",
      "It: 25400, Loss: 6.135547181429502e-08\n",
      "It: 25500, Loss: 5.2638302605600984e-08\n",
      "It: 25600, Loss: 3.86312279943013e-08\n",
      "It: 25700, Loss: 4.0158596448236494e-08\n",
      "It: 25800, Loss: 4.172321865780759e-08\n",
      "It: 25900, Loss: 3.449616059469918e-08\n",
      "It: 26000, Loss: 3.565100215041639e-08\n",
      "It: 26100, Loss: 3.837046236299102e-08\n",
      "It: 26200, Loss: 3.792342795350123e-08\n",
      "It: 26300, Loss: 3.8593981344092754e-08\n",
      "It: 26400, Loss: 4.183498347742898e-08\n",
      "It: 26500, Loss: 3.07708809543783e-08\n",
      "It: 26600, Loss: 2.600251214346372e-08\n",
      "It: 26700, Loss: 3.8668495960791915e-08\n",
      "It: 26800, Loss: 3.6433323913342974e-08\n",
      "It: 26900, Loss: 2.6151523613293648e-08\n",
      "It: 27000, Loss: 2.589075620562653e-08\n",
      "It: 27100, Loss: 2.5369216061221778e-08\n",
      "It: 27200, Loss: 2.0712606740858064e-08\n",
      "It: 27300, Loss: 3.2037487329716896e-08\n",
      "It: 27400, Loss: 2.2128215704242393e-08\n",
      "It: 27500, Loss: 2.6933840047149715e-08\n",
      "It: 27600, Loss: 1.572071894884175e-08\n",
      "It: 27700, Loss: 2.0600845473950358e-08\n",
      "It: 27800, Loss: 1.9110734328364742e-08\n",
      "It: 27900, Loss: 1.7434354404599617e-08\n",
      "It: 28000, Loss: 1.892447087925575e-08\n",
      "It: 28100, Loss: 1.333653543156288e-08\n",
      "It: 28200, Loss: 1.5534459052446437e-08\n",
      "It: 28300, Loss: 1.0244545656235005e-08\n",
      "It: 28400, Loss: 1.318752573808979e-08\n",
      "It: 28500, Loss: 1.4118846536348428e-08\n",
      "It: 28600, Loss: 1.5012917131684844e-08\n",
      "It: 28700, Loss: 9.313224857976365e-09\n",
      "It: 28800, Loss: 9.909270737296083e-09\n",
      "It: 28900, Loss: 1.2181698316737766e-08\n",
      "It: 29000, Loss: 1.080334044445408e-08\n",
      "It: 29100, Loss: 1.080334044445408e-08\n",
      "It: 29200, Loss: 1.1362134344494734e-08\n",
      "It: 29300, Loss: 1.0281800300049326e-08\n",
      "It: 29400, Loss: 7.823108383320232e-09\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "for iteration in range(1000000):\n",
    "    x_batch, y_batch = generate_xor_batch(batch_size=batch_size)\n",
    "    x_batch_time = np.swapaxes(x_batch, 0, 1)\n",
    "    y_batch_time = np.swapaxes(y_batch, 0, 1)\n",
    "    \n",
    "    def compute_loss():\n",
    "        y_batch_time_pred = run_rnn(net, x_batch_time)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(y_batch_time, y_batch_time_pred)\n",
    "        return loss\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = compute_loss()\n",
    "        \n",
    "    if iteration % 100 == 0:\n",
    "        print(\"It: {}, Loss: {}\".format(iteration, loss_value))\n",
    "    \n",
    "    variables = net.trainable_variables + output_net.trainable_variables\n",
    "    grads = tape.gradient(loss_value, variables)\n",
    "    optimizer.apply_gradients(zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
